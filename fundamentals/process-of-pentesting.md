# Process of Pentesting

## Scoping

Before begining an engagement, it is vitally important that both sides are completely clear about what will happen, and when it will happen. This effectively amounts to the client providing the pentester(s) with a list of targets, things to look out for, things to avoid, and any other relevant information about the assignment. In turn, the assessing team will establish whether the clinet's request is possible to fulfil, then either work with the client to find a more suitable scope or move on to arrange a period of time when the testing will be carried out. Additionally, the pentesters will also provide the client with the IP addresses the attackes will be coming from.

This process is refered to as scoping.

Aside from the scoping meetings the client will also provide the testing team with a point of contact in the company. This person will work with the team to some extent throughout the testing. In many cases this may simply be the person to reach out to should something go wrong; in other cases there may be daily, or even hourly reporting to this indivudual.

The pentesting methodologies are black box, grey box, and white box testing. In a purely black box penetration test the assessing team will be given no information about the targets, aside from addresses or an address range to attack. In extreme cases the attackers may be given little more than the company name and be forced to determine the addresses for themselves. In short, the attackers start with no prior information and have to perform initial enumeration for themselves from the same starting position as a bad actor (a malicious hacker, or goup of hackers, attacking the target without permission). This is good from a realism perspective, however, pentests are expensive and many companies do not wish to pay the assessors to sit around and perform initial footprinting of the organisation.

At the opposite end of the spectrum is white box penetration testing. As expected, in a white box penetration test, the attackers are given all relevant information about the target(s), which they can review in order to find vulnerabilities based on prior security knowledge and experience.

Most common are grey box tests where only some of the relevant information is provided by the client. The amount disclosed is dependent on the client and the target, meaning that a grey box test could fall anywhere on a sliding scale between white and black box tests.

The most common types of penetration test are web application and network pentests.

- Web application penetration testing revolves (as the name would suggest) around searching for vulnerabilities in web applications. In this style of assessment, the scope would provide the pentesters with a webapp (or multiple webapps) to work with. In a white box webapp pentest, the source code for the application would usually also be disclosed. Assessors would then attempt to find vulnerabilities in the application(s) over a period of time; following a methodology such as that outlined in the [OWASP Testing Guide](https://owasp.org/www-project-web-security-testing-guide/stable)
- Network pentest (often called infrastructure pentests) can be further split into two categories: internal and external
  - External network pentests are when the client provides a public-facing endpoint (such as VPN server or firewall) and asks the pentesters to assess it from the outside. Should the assessors succeed in gaining access, a further consultation with the client would be required to discuss an extension of the scope to include internal targets.
  - Internal network pentests usually involve a pentester physically going to the client and attacking the network from on-site, although remote internal pentests where companies give the pentester remote access to amachine in the network (e.g. via VPN) are growing in popularity. These are relatively common as companies often want to test their active directory infrastructure.

## Procedure

When a vulnerability is found in a target, there need to be astandardised way of evaluating and judging the severity of vulnerabilities.

The Common Vulnerability Scoring System is an open framework originally developed by the United States National Infrastructure Advisory Council (NIAC). It has since passed into the care of the Forum of Incident Response and Security Teams (FIRST); a global collaborative who have been maintaining the system since 2005. The short version is: the CVSS scoring system gives us a common method for calculating vulnerability scores which we can then share with a client.

## Enumeration - Outline

With the scope planned out, the day of the engagement is upon us!

It's time to start the testing. In hacking (as with everything), information is power. The more we know about the target, the more options we have available to us; thus we start with various kinds of enumeration.

We would often start with a passive footprinting stage before beginning the active enumeration that you may be familiar with. This would be time spent performing gathering OSINT (Open-Source Intelligence) about the target from their online footpring. For example, we may look for public email addresses, employee names, interesting subdomains/subdirectories in websites, Github repositories, or anything-else that are publicly available and may come in handy. Tools like [TheHarvester](https://tools.kali.org/information-gathering/theharvester) and the [Recon-ng](https://tools.kali.org/information-gathering/recon-ng) framework may come in handy for this.

## Enumeration - Port Scanning

Active scans like -T5 or -A switches for nmap is good for lab environment but is less likely to go well in the real world. In reality, fast and furious enumeration is much more likely to damage a target unnecessarily (the point can be made that if a server is unable to stand up to a port scanner then it isn't fit for purpose, but do you really want to explain to the client and your boss why the company website has gone down?). The mantra "slow and steady wins the race", comes to mind. Realistically, in today's world anything other than a small, slow, home-brew port scanner will be picked up by most intrusion detection systems very quickly indeed; however, we may as well minimise our own footprint as much as possible.

Quick scans with a small scope can be used to get an initial idea of what's available. Slower scans with a larger scope can then be run in the background whilst you look into the results from the initial scans. The goal should be to always have something running in the background whilst you focus on something else (a philosopy which shouldn't just apply to initial enumeration).

We can see that the server doesn't respond to ping, but we know the machine is active, so there is a firewall between us and the server. With that in mind let's do a quick TCP SYN scan against the top 1000 most common TCP ports on the target. If not already running as root, we will do this with **sudo** so that we can use a SYN "Stealth" scan (which is default for the root user):

```bash
$sudo nmap -vv ip -oN Initial-SYN-Scan
```

We have found 4 open ports 22, 53, 80 and 443, 22-ssh, 80-http, 443-https are common for a linux server but 53-dns is an interesting one.

Now lets scan for services.

```bash
$sudo nmap -p 22,53,80,443 -sV -Pn -vv target_ip -oN service-scan
```

We have found the versions of discovered services of 3 ports but the banner for DNS server is spoofed. Let's perform an UDP scan to confirm port 53 is indeed open. We go for scanning top 50 UDP ports.

```bash
$sudo nmap -sU --top-ports 50 -Pn -vv --open target_ip -oN udp-top-ports
```

Much like the **filtered** response from a TCP scan referring to a firewall in play, the **open|filtered** response in a UDP scan indicates a possible firewall. As the scan indicates, the three ports showing this state provided no response to the scan. This could mean that there is afirewall preventing access to the ports, or it could mean that the ports are open and just don't return a response (as is frequently the case with UDP). In short, UDP scans are not very accurate, but we have confirmed that UDP/53 is open.

### To summarise, based on initial information, we know for sure that there are three services running: SSH on TCP port 22, DNS on TCP and UDP ports 53 (with a modified banner), and HTTP(s) on TCP 80 and 443

## Enumeration - Vulnerability Scanning

With the initial enumeration done, let's look at some vulnerability scanning.

We could use Nmap for this (making use of the NSE - Nmap Scripting Engine);

```bash
$sudo nmap --script=vuln target_ip -oN vuln-scan
```

or we could do the more common thing and switch to an industry-standard vulnerability scanner: Nessus.

Vulnerability scanner are used to scan target (or usually a wide range of targets across a client network), checking for vulnerabilities against a central database. They will usually provide a list of discovered vulnerabilities, ranked from critical down to low or informational, with options to filter the results and export them into a report. There are a variety of vulnerability scanners available, including the opensourse [OpenVAS Framework](https://openvas.org), however, [Nessus](https://tenable.com/products/nessus) is one of the most popular vulnerability scanners currently available when it comes to industry usage. Go [here](https://tryhackme.com/room/openvas) To learn about OpenVAS and [here](https://tryhackme.com/room/rpnessusredux) to learn about Nessus.

As it happens, none of the vulnerabilities scan results are useful to us in terms of exploiting the target further (both medium vulnerabilities being to do with the self-signed SSL cert for the server, and the low vulnerability relating to a weak cipher enabled on SSH); however, they would definitely be worth reporting to the client.

## Enumeration - Web App: Initial Thoughts

Of the three services available, the webserver is the one most likely to have vulnerabilities that Nessus couldn't find. As the client has not asked us to focus specifically on the webapp, but rather on the server as a whole, we will not do a deep-dive analysis on the website(s) being served by the webserver. We can always discuss adding a full web application pentest to the scope with the client later on.

Nginx is easy to misconfigure, and any custom webapps on the server could potentially have vulnerabilities that Nessus is unable to detect. At this point we don't know if Nginx is being used as a reverse proxy, or if it has its PHP engine installed and enabled.

Navingating to the target IP address in Firefox gives us a message

```html
Host Name: ip, not found.
This server hosts sites on the hipflasks.thm domain.
```

This is the same for both the HTTP and HTTPS versions of the page. Aside from the overly verbose error message (which in itself is unnessary information exposure and should be rectified), we also learn that the client's domain appears to be **himflasks.thm**.

### Set-Up Firefox to resolve domain name for the machine

Editing the system-wide DNS servers for a split-tunnel VPN connection linke the one used for TryHackMe is, frankly, a colossal pain in the rear end. Fortunately there is an easier "hack" version using the FireFox config settings. This wil only allow FireFox to use the DNS server, but right now that's all we nee.

1. Navigate to about:confg in the FireFox search bar and accept the risk notice.
2. Search for network.dns.forceResolve, double click it and set the value to the IP address of the target machine, then click the tick button to save the setting:

## Enumeration - DNS

We still don't actually know exactly what DNS server is in use here; however, there are very few current vulnerabilities in Linux DNS servers, so the chances are that if there's something to be found, it will be a misconfiguration.

Fortunatelyu for us, misconfigurations in DNS are notoriously eaasy to make.

As the address system of the internet, it need not be said how important DNS is. As a result of this importance, it is good practice to have at least two DNS servers containing the records for a zone (or domain, in normal terms). This means that if one server goes down, there is still at least one other which contains the records for the domain; but this poses a problem: how do you update DNS records for the zone without having to go and update every server manually? The answer is something called a "Zone Transfer". In short: one server is set up as the master (or primary) DNS server. This server contains the primary records for the zone. In BIND9, zone configuration files for a primary server look something like this:

```bash
zone "example.com" IN {
    type master;
    file "/etc/bind/db.cxample.com";
    allow-query {any;};
    allow-transfer {172.16.0.2};
}
```

This defines a master zone for the domain example.com, it tells BIND to read the records from a file called ```/etc/bind/db.examples.com``` and accept queries from anywhere. Crucially, it also allows zone transfers to an IP address: ```172.16.0.2```.

In addition to the primary DNS server, one or more "slave" (or secondary) DNS servers are set up. They would have a zone file looking like this

```bash
zone "example.com" IN {
    type slave;
    file "/etc/bind/db.example.com";
    masters {172.16.0.1; };
    allow-transfer {none; };
};
```

This defines a slave zone, setting the IP address of the primary DNS server in the masters {}; directive.

### Zone Transfer

-------------------------------------------------------------------------------------------------;

Zone transfers allow secondary DNS servers to replicate the records for a zone from a primary DNS server. At frequent intervals (controlled by the Time To Live value of the zone), the secondary server(s) will query a serial number for the zone from the primary server. If the number is greater than the number that the secondary server(s) have stored for the zone then they will initiate a zone transfer, requesting all of the records that the primary server holds for that zone and making a copy locally. In some configurations a "DNS Notify List" may also exist on the primary DNS server. If this is in place then the primary server will notify all of the secondary servers whenever a change is made, instructing them to request a zone transfer.

--------------------------------------------------------------------------------------------------;

What if a DNS server has an entry in the zone config which looks like this: ```allow-transfer {any; };``` Rather than specifying a specific IP address (or set of IP addresses), the server allows any remote machine to request all of the records for the zone. This means that if the server is configured incorrectly we may be able to dump every record for the domain including the subdomains we are looking for here!

Zone transfers are initiated by sending the target DNS server an axfr query. This can be done in a variety of ways, however, on Linux it is easiest to use either the dig or host commands.

```bash
$dig axfr hipflasks.thm @10.10.81.95
#or
host -t axfr hipflasks.thm 10.10.81.95
```

## Enumeration - Web App Fingerprinting and Enumeration

Having a look around the page and in the source code, there don't appear to be any working links, so if we want to access other pages then we will need to look for them ourselves. Of course, directory listing is disabled, which makes this slightly harder.

The source code does indicate the presentce of **assets/**, **assets/img/**, **css/**, and **js/** subdirectories, which seem to contain all of the static assets in use on the page

Having a look through some of the font stylesheets reveals that there is also an **assets/fonts/** subdirectory. E.g. in **css/railway.css**

```css
@font-face {
    font-family: 'Raleway';
    font-style: italic;
    font-weight: 100;
    src: url(/assets/fonts/1Pt_....JQ.ttf) format('truetype');
}
```

Nothing ground breaking so far, but we can start to build up a map of the application from what we have here:

```bash
/
|__assets/
|____imgs/
|____fonts/
|__css/
|__js/
```

Now we can use BurpSuite to intercept the response from the server and can find some interesting info from the header.

First of all, the server header: waitress. This would normally be Nginx, as we already know from the TCP fingerprint that this is the webserver in use. This means that we are dealing with a reverse proxy to a waitress server. A quick Google search for waitress web app tells us that Waitress is a production-ready Python WSGI server; in other words, we are most likely dealing with either a Django or a Flask webap, these being the most popular Python web-development frameworks.

Secondly, there are various security-headers in play here, however, notably absent are the **Content-Security-Policy** and **X-XSS-Protection** headers, meaning that the site may be vulnerable to XSS, should we find a suitable input field. Equally, the HSTS (Http Strict Transport Security) header which should usually force a HTTPS connection won't actually be doing anything here due to the self-signed certificate.

Before we go further, let's start a couple of scans to run in the background while we look around manually. Specifically, let's go for Nikto and Feroxbuster (or Gobuster), running in parallel.

```bash
$nikto --url https://hipper.hipflasks.thm | tee nikto
$feroxbuster -t 10 -u https://hiper.hipflasks.thm -k -w /usr/share/seclists/Discovery/Web-Content/common.txt -x py,html,txt -o feroxbuster
```

## Web App - Understanding the Vulnerability

Web apps traditionally follow the same structure asthe underlying file-system. For example, with a PHP web application, the root directory of the webserver would contain a file called **index.php**, and usually a few subdirectories related to different fundtions. There might then be a subdirecotry called **about/**, which would also contain an **index.php**. The index files are used to indicate the default content for that directory, meaning that if you tried to access **https://example.com/**, then the webserver would likely actually be reading a file called **/var/www/html/index.php**. Accessing **https://example.com/about/**, would be reading **/var/www/html/about/index.php** from the filesystem.

This approach makes life very easy for us as hackers. if a file is under the webroot (**/var/www/html** by default for Apache on Linux) then we will be able to access it from the websever.

Modern webapps are often not like this though -- they follow a design structure called "routing". Instead of the routes being defined by the structure of the file system, the routes are coded into the webapp itself. Accessing **https://example.com/about/** in a routed web app would be a result of a program running on the webserver (written in something like Python -- like our target application here -- NodeJS or Golang) deciding what page you were trying to access, then either serving a static file, or generating a dynamic result and displaying it to you. This approach practically eliminates the possibility of file upload vulnerabilities leading to remote code execution, and means that we can only access routes that have been explicitly defined. It's also a lot neater that the traditional approach from an organisational perspective.

There is a downside to routing, however. Serving static content such as CSS or front-end Javascript can be very tedious if you have to define a route for each page. Additionally, it's also realatively slow to have your webapp handling the static content for your (although most frameworks do have the option to server a directory). As such, it's very common to have a webapp sitting behind a reverse proxy such as Nginx or Caddy. The webserer handles the static content, and any requests that don't match the ruleset defined for static content get forwarded to the webapp, which then sends the response back through the proxy to the user.

What this means is that searching for file extensions in a route fuzzing attempt (like the Feroxbuster scan we ran) won't actually do anything with a routed application, unless the reverse proxy has been misconfigured to serve more static content than it's suposed to. Unfortunately, it is very easy to mess up the configuration, for a reverse proxy, for example, this common Nginx configuration could potentially leak the full source code for the webapp -- a very dangerous prospect.

```nginx
root /var/www/webapp;
location ^~ {
    try_files $uri $uri @proxypass;
}

location @proxypass {
    //Various proxy headers
    proxy_pass htt://127.0.0.1:8000;
}
```

This configuration fist looks for files in **/var/www/webapp** and its subdirectories. For example, if you were looking for *htts://example.com/assets/css/style.css* then Nginx would look for **/var/www/webapp/asstes/css/style.css/**. Notice that this is identical behaviour to an non-routed webapp.

If the file exists then Nginx will serve it and the request will never reach the webapp. If the file does not exist then the request gets sent to the named location block: **proxypass**, which results in it getting passed to the webapp running on **127.0.0.1:8000**.

This is all well and good, but what happens if the source code for the webapp is also stored in **/var/www/webapp**? A request to **/var/www/webapp/app.py**, for example may leak the source code for the webappp, as Nginx would see that the file exists and serve it as plaintext before the request even reaches the webapp. An example application structure may look something like this:

```bash
/
|__app.py
|__assets/
|____css/
|______style.css
|____js/
|______scripts.js
|____app_modules/
|______database/
|________connection.py
```

This would result in Nginx serving the assets directory, yes, but it would also be serving all of the Python files.

A better solution would be to use a configuration such as this:

```nginx
root /var/www/webapp;
location ^~ /assets {
    alias /var/www/webapp/assets;
}

location / {
    //Various proxy headers
    proxy_pass http://127.0.0.1:8000;
}
```

This would take any requests to **/assets/\*** and attempt to serve the static files. Anything else would just get passed straight to the webapp.

There are million-and-one different ways to accomplish the same objective with Nginx configuration files -- many of them will have vulnerabilities like this, many will not. It all depends on the experience of the sysadmin. As such, searching for **.py** file with feroxbuster is still an effective strategy when we know that tehere is a reverse proxy in front of our Python webapp -- even with a routed application.

## Web App - Full Source Code Disclosure

Let's focus on gathering more information about the application; using our discovered file to grab the rest of the code seems like a good start. Flask applications work by having one main file. This file then imports everything else that the application needs to runn -- for example, blueprings that map out other parts of the app, authentication modules, etc.

This means that we don't need to do any more fuzzing to find the rest of the source code: we can just read what the main.py file is importing and pull on the metaphorical thread until we have all of the files downloaded. Whenever we find a new file, we should download a copy locally using the curl **-o Filename** switch so that we can review the source code in detail later.

Let's start by looking at what the **main.py** file is importing:

```python
from flask import Flask, redirect, render_template, request, session
from datetime import datetime
from waitress import serve
from modules import abp
from libs.db import AuthConn, StatsConn
```

A lot of these are just standard Python modules (which we can chekc by Googling them), but the last two lines are reffering to custom modules.

The syntax here tells us a lot. Starting with the first line of interest (```from modules import abp```), we can see that it's importing an object called abp (which, looking further down the code appears to be a Bluepring) from a modules file. This could mean one of two file structures:

- There is a file calles modules.py in the webroot.
- There is a directory called modules in the webroot which contains a file called **__init__.py** -- a file effectively used to initialise a new module inside a directory.

Only one way to find out which it is. Lets try both!

Note: -k switch must be used with curl command to ignore the self-signed certificate.

CURLing **https://hipper.hipflasks.thm/modules.py** gives us 404 error, so it must be the second option.

CURLing **https://hiper.hipflasks.thm/modules/__init__.py** gives us what we're looking for. The file contains a single line 

```python
from modules.admin import abp
```

As expected, the init (initialisation) file is importing the abp Blueprint object from another Python file in the directory: **admin.py**

CURLing **https://hipper.hipflasks.thm/modules/admin.py** give us a much chunkier file

Once again we have some imports we can look into:

```python
from libs.auth import authCheck, checkAuth
from libs.db import AuthConn, StatsConn
```

The libs.db import is the same as the second import in main.py, but we can add libs.auth to our list of things to check.

We know that a libs/ subdirectory exists (it has to be a subdirectory if we're importing two different modules from it), but we don't know if the two files we know of (auth and db) are Python files, or directories.

We can establish this in the same way as before -- first checking to see if **libs/auth.py** exists, then if that fails , checking to see if **libs/auth/__init__.py** exists.

When attempting to cURL **https://hipper.hipflasks.thm/lib/auth.py** we receive a 200 response and a Python file, so this is clearly the correct path.

Looking at the other items in the libs/ subdirectory, we can quickly ascertain that this is a directory by the presence of a **libs/db/__init__.py** file:

```python
from libs.db.base import Conn
from libs.db.auth import AuthConn
from libs.db.stats import StatsConn
```

Three imports means three different files -- either more directories or Python files.

Using the same method as before we find that these three are python files. However, there are a few more things we can fill in.

First, from the **main.py** file we can see that the app's template folder has been set to "views":

```python
app.template_folder="views"
```

The template folder contains static HTML templates which Flask uses to create dynamic responses. For example, in line 24 of **main.py**, we can see an example of the Flask **render_template** function where it passes in the current year to be used for the copyright notice in the index.html template.

```python
main.py, line 24
return render_template("index.html", year=datetime.now().date.strftime("%Y"), 200)
```

Similarly, looking at the files in **libs/db/** tells us that there is a directory called **data/** containing two SQLite3 databases: **users.db** and **stats.db**.

So our final structure diagram now looks like this:

```bash
/
|__app.py
|__assets/
|____imgs/
|____fonts/
|__css/
|____style.css
|__js/
|____scripts.js
|__modules/
|____ __init__.py
|____admin.py
|__libs/
|____auth.py
|____db/
|______base.py
|______auth.py
|______stats.py
|__views/
|____index.html
|__data/
|____users.db
|____stats.db

```

## Web App - Implication of the Vulnerability

Because HTTP(S) is inherently stateless, websites store information which needs to persists between requests in cookies -- tiny little pieces of information stored on your computer. Unfortunately, this also poses a problem: if the information is stored on your computer, what's stopping you from just editing it? When it comes to sessions, there are two mainstream solutions.

Sessions are a special type of cookie -- they identify you to the website and need to be secure. Sessions usually hold more information than just a single value (unlike a standard cookie where there may only be a single value stored for each index). For example, if you are logged into a website then your session may contain your user ID, privilege levels, full name, etc. It's a lot quicker to store these things in the session than it is to constantly query the database for them!

So, how do we keep sessions secure? There are two common schools of thought when it comes to session storage:

- **Server Side Session Storage:** store the session information on the server, but give the client a cookie to identify it.
  - This is the method which PHP and most other traditional languages use. Effectively, when a session is created for a client (i.e avisitor to the site), the client is given a cookie with a unique identifier, but none of the session information is actually handed over to the client. Instead the server stores the session information in a file locally, identified by the same unique ID. When the client makes a request, the server reads the ID and selects the correct file from the disk, reading the information from it. This is secure because there is no way for the client to edit the actual session data (so there is not way for them to elevate their privileges, for example).
  - There are other forms of server side session storage (e.g. storing the data in a Redis or memcached server rather than on disk), but the principle is the same.
- **Client Side Session Storage:** store all of the session information in the client's cookies, but encrypt of sign it to ensure that it can't be tampered with.
  - In a client side session storage situation, all of the session values are stored directly within the cookie -- usually in something like a JSON Web Token (JWT). This is the method Flask uses. The cookie is sent off with each request as normal and is read by server, exactly as with any other cookie -- only with an extra layer of security added in. By either signing or encrypting the cookie with a private secret known only to the server, the cookie in theory cannot be modified. Flask signs its cookies, which means we can actually decode them without requiring the key.

## Web App - Source Code Review

Now that we have a copy of the source code for the site, we have effectively turned the webapp segment of this assessment into a white-box testing.

Let's start by looking at **modules/admin.py**. This contains the code defining the admin section -- if we look at this then we will see what authentication measures are in place:

```python
#!/usr/bin/python3
from flask import Blueprint, render_template_string, request redirect, session, abort, url_for, flash, get_flashed_messages
from libs.auth import authCheck, checkAuth
from libs.db import AuthConn, StatsConn

abp = Blueprint("abp", __name__)

@abp.route("/")
@authCheck
def manageHome():
    conn = StatsConn()
    uniqueViews = conn.getViews()
    response = f"""..........."
```

Right at the top of the file we find what we're looking for. Specifically, thereis one line of code which handles the authentication for the /admin route:

@authCheck

Imported in:

```python
from libs.auth import authCheck, checkAuth
```

This is what is referred to as a decorator -- a function with wraps around another function to apply pre-processing.

## Web App - Cookie Forgery

## Web App - Server-Side Template Injection (SSTI)

## Web App - SSTI -> RCE

## Enumeration - Shell Stabilisation and Local Enumeration

## Privilege Escalation - Polkit

## Wrapping Up
